#!/usr/bin/env python3
"""
AIäº‹æ¥­ ç©¶æ¥µã®å®Œå…¨ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆä½œæˆ
å…¨444ã‚«ãƒ©ãƒ ã‚’å«ã‚€å®Œå…¨ç‰ˆ
"""
import pandas as pd
import json
import re
from pathlib import Path
from typing import Dict, List, Set, Any
from collections import defaultdict
import time
import warnings
warnings.filterwarnings('ignore')


class AIUltimateSpreadsheetGenerator:
    """AIäº‹æ¥­ç©¶æ¥µã®ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆç”Ÿæˆã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, feather_dir: str = "data/full_feather"):
        self.feather_dir = Path(feather_dir)
        self.output_dir = Path("data/ai_ultimate_spreadsheet")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # åŸºæœ¬å½¢AIãƒ‘ã‚¿ãƒ¼ãƒ³
        self.basic_ai_patterns = [
            r'\bAI\b',     # åŠè§’AIï¼ˆå˜èªå¢ƒç•Œã‚ã‚Šï¼‰
            r'\bï¼¡ï¼©\b',    # å…¨è§’AIï¼ˆå˜èªå¢ƒç•Œã‚ã‚Šï¼‰
            r'(?<![a-zA-Z])AI(?![a-zA-Z])',  # ã‚ˆã‚Šç²¾å¯†ãªåŠè§’AI
            r'(?<![ï½-ï½šï¼¡-ï¼º])ï¼¡ï¼©(?![ï½-ï½šï¼¡-ï¼º])'  # ã‚ˆã‚Šç²¾å¯†ãªå…¨è§’AI
        ]
        
        self.tables_data = {}
        self.metadata = {}
        self.column_mapping = {}
        self.load_metadata()
    
    def load_metadata(self):
        """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        print("Loading metadata...")
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        metadata_path = self.feather_dir / 'full_feather_metadata.json'
        if metadata_path.exists():
            with open(metadata_path, 'r', encoding='utf-8') as f:
                self.metadata = json.load(f)
        
        # ã‚«ãƒ©ãƒ ãƒãƒƒãƒ”ãƒ³ã‚°èª­ã¿è¾¼ã¿
        column_mapping_path = self.feather_dir / 'column_mapping.json'
        if column_mapping_path.exists():
            with open(column_mapping_path, 'r', encoding='utf-8') as f:
                self.column_mapping = json.load(f)
        
        print(f"  Found metadata for {len(self.column_mapping)} tables")
        print(f"  Total columns available: {sum(info['column_count'] for info in self.column_mapping.values())}")
    
    def load_feather_tables(self):
        """å…¨ã‚«ãƒ©ãƒ ä¿æŒã®Featherãƒ†ãƒ¼ãƒ–ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        print("\\nLoading complete Feather tables...")
        
        total_columns = 0
        
        for table_name, info in self.column_mapping.items():
            feather_path = self.feather_dir / f"{table_name}.feather"
            
            if feather_path.exists():
                print(f"  Loading: {table_name} ({info['japanese_name']})")
                try:
                    df = pd.read_feather(feather_path)
                    self.tables_data[table_name] = df
                    total_columns += len(df.columns)
                    print(f"    Records: {len(df):,}, Columns: {len(df.columns)}")
                except Exception as e:
                    print(f"    Error loading {table_name}: {e}")
            else:
                print(f"  Warning: {feather_path} not found")
        
        print(f"\\nLoaded {len(self.tables_data)} tables with {total_columns} total columns")
    
    def is_basic_ai_match(self, text: str) -> List[Dict]:
        """åŸºæœ¬å½¢AIãƒãƒƒãƒã‚’ãƒã‚§ãƒƒã‚¯"""
        if not text or pd.isna(text):
            return []
        
        text_str = str(text)
        matches = []
        
        for pattern in self.basic_ai_patterns:
            try:
                found = re.finditer(pattern, text_str, re.IGNORECASE)
                for match in found:
                    matched_text = match.group()
                    if matched_text.upper() in ['AI', 'ï¼¡ï¼©']:
                        matches.append({
                            'pattern': pattern,
                            'matched_text': matched_text,
                            'position': f"{match.start()}-{match.end()}"
                        })
            except re.error:
                continue
        
        return matches
    
    def find_ai_projects(self) -> Set[int]:
        """åŸºæœ¬å½¢AIäº‹æ¥­ã‚’ç‰¹å®šï¼ˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®æ¤œç´¢ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’ä½¿ç”¨ï¼‰"""
        print("\\nFinding AI projects using comprehensive search...")
        
        ai_projects = set()
        search_summary = defaultdict(int)
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ¤œç´¢ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’å–å¾—
        ai_search_fields = self.metadata.get('ai_search_fields', {})
        
        for table_name, search_fields in ai_search_fields.items():
            if table_name not in self.tables_data:
                continue
            
            df = self.tables_data[table_name]
            print(f"  Searching in {table_name} ({self.column_mapping[table_name]['japanese_name']})...")
            
            available_fields = [f for f in search_fields if f in df.columns]
            if not available_fields:
                print(f"    No searchable fields found")
                continue
            
            matches_found = 0
            
            for idx, row in df.iterrows():
                # äºˆç®—äº‹æ¥­IDã®å–å¾—
                project_id = None
                for col in ['äºˆç®—äº‹æ¥­ID', 'äºˆç®—äº‹æ¥­ã‚³ãƒ¼ãƒ‰', 'äº‹æ¥­ID']:
                    if col in row and pd.notna(row[col]):
                        try:
                            project_id = int(row[col])
                            break
                        except (ValueError, TypeError):
                            continue
                
                if not project_id:
                    continue
                
                # AIæ¤œç´¢
                for field in available_fields:
                    text = row.get(field, '')
                    if self.is_basic_ai_match(text):
                        ai_projects.add(project_id)
                        matches_found += 1
                        search_summary[table_name] += 1
                        break
            
            print(f"    Found {matches_found} AI matches")
        
        print(f"\\nAI Search Summary:")
        for table_name, count in search_summary.items():
            print(f"  {table_name}: {count} matches")
        print(f"Total unique AI projects: {len(ai_projects)}")
        
        return ai_projects
    
    def collect_ultimate_data(self, project_ids: Set[int]) -> pd.DataFrame:
        """å…¨444ã‚«ãƒ©ãƒ ã®AIäº‹æ¥­ãƒ‡ãƒ¼ã‚¿ã‚’åé›†"""
        print(f"\\nCollecting ULTIMATE data for {len(project_ids)} projects...")
        print("Including ALL 444 columns from ALL tables...")
        
        all_project_data = []
        column_stats = defaultdict(int)
        
        for project_id in sorted(project_ids):
            project_record = {
                'äºˆç®—äº‹æ¥­ID': project_id,
                'AI_æ¤œå‡º_è©³ç´°': '',
                'AI_ãƒãƒƒãƒ_æ•°': 0
            }
            
            ai_matches_detail = []
            total_ai_matches = 0
            
            # å…¨ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰å…¨ã‚«ãƒ©ãƒ ã®ãƒ‡ãƒ¼ã‚¿ã‚’åé›†
            for table_name, df in self.tables_data.items():
                # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆIDã‚«ãƒ©ãƒ ã‚’ç‰¹å®š
                id_col = None
                for col in ['äºˆç®—äº‹æ¥­ID', 'äºˆç®—äº‹æ¥­ã‚³ãƒ¼ãƒ‰', 'äº‹æ¥­ID']:
                    if col in df.columns:
                        id_col = col
                        break
                
                if not id_col:
                    continue
                
                # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
                try:
                    project_df = df[df[id_col] == project_id]
                except:
                    continue
                
                if project_df.empty:
                    # ãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆã‚‚å…¨ã‚«ãƒ©ãƒ ã‚’ç©ºå€¤ã§è¿½åŠ 
                    for col in df.columns:
                        if col not in ['äºˆç®—äº‹æ¥­ID', 'äºˆç®—äº‹æ¥­ã‚³ãƒ¼ãƒ‰', 'äº‹æ¥­ID']:
                            prefixed_col = f"{table_name}_{col}"
                            project_record[prefixed_col] = ''
                    continue
                
                # å…¨ã‚«ãƒ©ãƒ ã®ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ ï¼ˆä¾‹å¤–ãªãå…¨ã¦ï¼‰
                for col in df.columns:
                    if col in ['äºˆç®—äº‹æ¥­ID', 'äºˆç®—äº‹æ¥­ã‚³ãƒ¼ãƒ‰', 'äº‹æ¥­ID']:
                        continue
                    
                    prefixed_col = f"{table_name}_{col}"
                    
                    # è¤‡æ•°ãƒ¬ã‚³ãƒ¼ãƒ‰ãŒã‚ã‚‹å ´åˆã®å‡¦ç†
                    values = project_df[col].dropna()
                    
                    if len(values) > 0:
                        try:
                            # æ•°å€¤å‹ã®å ´åˆ
                            if pd.api.types.is_numeric_dtype(values):
                                unique_values = values.unique()
                                if len(unique_values) == 1:
                                    project_record[prefixed_col] = str(unique_values[0])
                                else:
                                    project_record[prefixed_col] = ' | '.join(map(str, unique_values))
                                column_stats[prefixed_col] += 1
                            else:
                                # æ–‡å­—åˆ—å‹ã®å ´åˆ
                                str_values = values.astype(str).unique()
                                clean_values = [v for v in str_values if v and v != 'nan']
                                if clean_values:
                                    project_record[prefixed_col] = ' | '.join(clean_values)
                                    column_stats[prefixed_col] += 1
                                else:
                                    project_record[prefixed_col] = ''
                        except Exception as e:
                            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯æ–‡å­—åˆ—ã¨ã—ã¦å‡¦ç†
                            str_values = values.astype(str).unique()
                            clean_values = [v for v in str_values if v and v != 'nan']
                            project_record[prefixed_col] = ' | '.join(clean_values) if clean_values else ''
                    else:
                        project_record[prefixed_col] = ''
                
                # AIæ¤œå‡ºè©³ç´°ã®åé›†
                ai_search_fields = self.metadata.get('ai_search_fields', {})
                if table_name in ai_search_fields:
                    search_fields = ai_search_fields[table_name]
                    available_fields = [f for f in search_fields if f in df.columns]
                    
                    for idx, row in project_df.iterrows():
                        for field in available_fields:
                            text = row.get(field, '')
                            matches = self.is_basic_ai_match(text)
                            if matches:
                                for match in matches:
                                    ai_matches_detail.append(f"{table_name}.{field}: {match['matched_text']}")
                                    total_ai_matches += 1
            
            # AIæ¤œå‡ºè©³ç´°ã‚’è¿½åŠ 
            project_record['AI_æ¤œå‡º_è©³ç´°'] = ' | '.join(ai_matches_detail)
            project_record['AI_ãƒãƒƒãƒ_æ•°'] = total_ai_matches
            
            all_project_data.append(project_record)
        
        # DataFrameã«å¤‰æ›
        df_ultimate = pd.DataFrame(all_project_data)
        
        # ã‚«ãƒ©ãƒ ã‚’æ•´ç†ï¼ˆAIé–¢é€£ã€ãƒ†ãƒ¼ãƒ–ãƒ«é †ï¼‰
        ai_cols = ['äºˆç®—äº‹æ¥­ID', 'AI_æ¤œå‡º_è©³ç´°', 'AI_ãƒãƒƒãƒ_æ•°']
        
        # ãƒ†ãƒ¼ãƒ–ãƒ«é †ã«ã‚«ãƒ©ãƒ ã‚’æ•´ç†ï¼ˆã‚«ãƒ†ã‚´ãƒªé †ï¼‰
        table_order = ['organizations', 'projects', 'policies_laws', 'subsidies', 'related_projects',
                      'budget_summary', 'budget_items', 'goals_performance', 'goal_connections',
                      'evaluations', 'expenditure_info', 'expenditure_connections', 
                      'expenditure_details', 'contracts', 'remarks']
        
        other_cols = []
        for table in table_order:
            table_cols = [col for col in df_ultimate.columns if col.startswith(f"{table}_")]
            other_cols.extend(sorted(table_cols))
        
        # æœ€çµ‚çš„ãªã‚«ãƒ©ãƒ é †åº
        ordered_cols = ai_cols + other_cols
        existing_cols = [col for col in ordered_cols if col in df_ultimate.columns]
        df_ultimate = df_ultimate[existing_cols]
        
        print(f"\\nUltimate data collection complete:")
        print(f"  Projects: {len(df_ultimate)}")
        print(f"  Total columns: {len(df_ultimate.columns)} (target: 444+3)")
        print(f"  Columns with data: {len([col for col, count in column_stats.items() if count > 0])}")
        
        # ãƒ†ãƒ¼ãƒ–ãƒ«åˆ¥ã‚«ãƒ©ãƒ æ•°ã‚’è¡¨ç¤º
        for table in table_order:
            table_cols = [col for col in df_ultimate.columns if col.startswith(f"{table}_")]
            if table_cols and table in self.column_mapping:
                japanese_name = self.column_mapping[table]['japanese_name']
                print(f"  {table} ({japanese_name}): {len(table_cols)} columns")
        
        return df_ultimate
    
    def generate_ultimate_summary(self, df: pd.DataFrame) -> Dict:
        """ç©¶æ¥µã®ãƒ‡ãƒ¼ã‚¿ã‚µãƒãƒªãƒ¼ã‚’ç”Ÿæˆ"""
        print("\\nGenerating ultimate data summary...")
        
        summary = {
            'basic_statistics': {
                'total_projects': len(df),
                'total_columns': len(df.columns),
                'data_columns': len(df.columns) - 3,
                'ai_metadata_columns': 3,
                'total_ai_matches': df['AI_ãƒãƒƒãƒ_æ•°'].sum(),
                'average_ai_matches': round(df['AI_ãƒãƒƒãƒ_æ•°'].mean(), 2),
                'max_ai_matches': df['AI_ãƒãƒƒãƒ_æ•°'].max()
            },
            'table_breakdown': {},
            'data_coverage': {},
            'ministry_distribution': {},
            'completeness_analysis': {}
        }
        
        # ãƒ†ãƒ¼ãƒ–ãƒ«åˆ¥ã‚«ãƒ©ãƒ æ•°ã¨å……å®Ÿåº¦
        for table_name, info in self.column_mapping.items():
            table_cols = [col for col in df.columns if col.startswith(f"{table_name}_")]
            if table_cols:
                # ãƒ‡ãƒ¼ã‚¿å……å®Ÿåº¦è¨ˆç®—
                coverage_stats = {}
                for col in table_cols:
                    non_empty = df[col].notna() & (df[col] != '')
                    coverage_pct = (non_empty.sum() / len(df)) * 100
                    coverage_stats[col] = round(coverage_pct, 1)
                
                avg_coverage = round(sum(coverage_stats.values()) / len(coverage_stats), 1) if coverage_stats else 0
                
                summary['table_breakdown'][table_name] = {
                    'japanese_name': info['japanese_name'],
                    'category': info['category'],
                    'total_columns': len(table_cols),
                    'original_columns': info['column_count'],
                    'average_coverage': avg_coverage,
                    'columns_with_data': len([col for col, pct in coverage_stats.items() if pct > 0])
                }
        
        # å…¨ä½“ã®ãƒ‡ãƒ¼ã‚¿å……å®Ÿåº¦ï¼ˆä¸Šä½50ã‚«ãƒ©ãƒ ï¼‰
        all_coverage = {}
        for col in df.columns:
            if not col.startswith('AI_') and col != 'äºˆç®—äº‹æ¥­ID':
                non_empty = df[col].notna() & (df[col] != '')
                coverage_pct = (non_empty.sum() / len(df)) * 100
                if coverage_pct > 0:
                    all_coverage[col] = round(coverage_pct, 1)
        
        # ä¸Šä½50ã‚«ãƒ©ãƒ ã‚’é¸æŠ
        top_coverage = dict(sorted(all_coverage.items(), key=lambda x: x[1], reverse=True)[:50])
        summary['data_coverage'] = top_coverage
        
        # åºœçœåºåˆ†å¸ƒ
        ministry_col = 'projects_åºœçœåº'
        if ministry_col in df.columns:
            ministry_counts = df[ministry_col].value_counts().head(20)
            summary['ministry_distribution'] = ministry_counts.to_dict()
        
        # å®Œå…¨æ€§åˆ†æ
        summary['completeness_analysis'] = {
            'perfect_records': len(df[df.notna().all(axis=1)]),  # å…¨ã‚«ãƒ©ãƒ ã«ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°
            'mostly_complete': len(df[df.notna().sum(axis=1) > len(df.columns) * 0.8]),  # 80%ä»¥ä¸Šã®ã‚«ãƒ©ãƒ ã«ãƒ‡ãƒ¼ã‚¿
            'basic_complete': len(df[df.notna().sum(axis=1) > len(df.columns) * 0.5]),  # 50%ä»¥ä¸Šã®ã‚«ãƒ©ãƒ ã«ãƒ‡ãƒ¼ã‚¿
            'total_non_null_cells': df.notna().sum().sum(),
            'total_possible_cells': len(df) * len(df.columns),
            'overall_completeness': round((df.notna().sum().sum() / (len(df) * len(df.columns))) * 100, 2)
        }
        
        return summary
    
    def save_ultimate_output(self, df: pd.DataFrame, summary: Dict):
        """ç©¶æ¥µã®ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‚’ä¿å­˜"""
        print("\\nSaving ULTIMATE spreadsheet...")
        
        # CSVã§ä¿å­˜ï¼ˆå…¨ã‚«ãƒ©ãƒ å¯¾å¿œï¼‰
        csv_path = self.output_dir / 'ai_ultimate_all_444_columns.csv'
        df.to_csv(csv_path, index=False, encoding='utf-8-sig')
        print(f"  CSV saved: {csv_path} ({len(df.columns)} columns)")
        
        # Parquetä¿å­˜ï¼ˆåŠ¹ç‡çš„ï¼‰
        parquet_path = self.output_dir / 'ai_ultimate_all_444_columns.parquet'
        df.to_parquet(parquet_path, index=False, compression='snappy')
        print(f"  Parquet saved: {parquet_path}")
        
        # Excelä¿å­˜ï¼ˆå¯èƒ½ãªå ´åˆï¼‰
        excel_path = self.output_dir / 'ai_ultimate_all_444_columns.xlsx'
        if len(df.columns) <= 16384:
            try:
                with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:
                    df.to_excel(writer, sheet_name='AIäº‹æ¥­å®Œå…¨ãƒ‡ãƒ¼ã‚¿', index=False)
                    
                    # ã‚µãƒãƒªãƒ¼ã‚·ãƒ¼ãƒˆ
                    summary_data = []
                    for key, value in summary['basic_statistics'].items():
                        summary_data.append([key, value])
                    summary_df = pd.DataFrame(summary_data, columns=['é …ç›®', 'å€¤'])
                    summary_df.to_excel(writer, sheet_name='åŸºæœ¬çµ±è¨ˆ', index=False)
                    
                    # ãƒ†ãƒ¼ãƒ–ãƒ«åˆ¥çµ±è¨ˆ
                    table_data = []
                    for table, stats in summary['table_breakdown'].items():
                        table_data.append([
                            table, stats['japanese_name'], stats['category'],
                            stats['total_columns'], stats['average_coverage']
                        ])
                    table_df = pd.DataFrame(table_data, 
                        columns=['ãƒ†ãƒ¼ãƒ–ãƒ«', 'æ—¥æœ¬èªå', 'ã‚«ãƒ†ã‚´ãƒª', 'ã‚«ãƒ©ãƒ æ•°', 'å¹³å‡å……å®Ÿåº¦'])
                    table_df.to_excel(writer, sheet_name='ãƒ†ãƒ¼ãƒ–ãƒ«åˆ¥çµ±è¨ˆ', index=False)
                
                print(f"  Excel saved: {excel_path}")
            except Exception as e:
                print(f"  Excel save error: {e}")
        else:
            print(f"  Excel column limit exceeded ({len(df.columns)} > 16384)")
        
        # å®Œå…¨ã‚«ãƒ©ãƒ ãƒªã‚¹ãƒˆä¿å­˜
        columns_path = self.output_dir / 'ultimate_columns_list.txt'
        with open(columns_path, 'w', encoding='utf-8') as f:
            f.write(f"AIäº‹æ¥­ ç©¶æ¥µã®å®Œå…¨ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ\\n")
            f.write(f"Total columns: {len(df.columns)}\\n")
            f.write("="*100 + "\\n\\n")
            
            # AIé–¢é€£ã‚«ãƒ©ãƒ 
            f.write("[AIé–¢é€£ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿] - 3 columns\\n")
            f.write("-"*50 + "\\n")
            for col in ['äºˆç®—äº‹æ¥­ID', 'AI_æ¤œå‡º_è©³ç´°', 'AI_ãƒãƒƒãƒ_æ•°']:
                if col in df.columns:
                    f.write(f"  {col}\\n")
            
            # ãƒ†ãƒ¼ãƒ–ãƒ«åˆ¥ã‚«ãƒ©ãƒ 
            for table_name, info in self.column_mapping.items():
                table_cols = [col for col in df.columns if col.startswith(f"{table_name}_")]
                if table_cols:
                    f.write(f"\\n[{table_name}] {info['japanese_name']} ({info['category']}) - {len(table_cols)} columns\\n")
                    f.write("-"*80 + "\\n")
                    for col in table_cols:
                        clean_col = col.replace(f"{table_name}_", "")
                        f.write(f"  {clean_col}\\n")
        
        print(f"  Column list saved: {columns_path}")
        
        # ã‚µãƒãƒªãƒ¼JSONä¿å­˜
        summary_path = self.output_dir / 'ultimate_summary.json'
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2, default=str)
        print(f"  Summary saved: {summary_path}")
        
        # HTMLãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        self.generate_ultimate_html_report(df, summary)
    
    def generate_ultimate_html_report(self, df: pd.DataFrame, summary: Dict):
        """ç©¶æ¥µã®HTMLãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        html_content = f"""<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <title>AIäº‹æ¥­ ç©¶æ¥µã®å®Œå…¨ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ</title>
    <style>
        body {{ font-family: 'Segoe UI', Arial, sans-serif; margin: 20px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); min-height: 100vh; }}
        .container {{ max-width: 1400px; margin: 0 auto; background: white; padding: 40px; border-radius: 15px; box-shadow: 0 10px 30px rgba(0,0,0,0.3); }}
        h1 {{ color: #2c3e50; text-align: center; font-size: 3em; margin-bottom: 20px; text-shadow: 2px 2px 4px rgba(0,0,0,0.1); }}
        .ultimate-banner {{ background: linear-gradient(135deg, #ff6b6b, #feca57, #48dbfb, #ff9ff3); padding: 30px; border-radius: 15px; text-align: center; color: white; font-size: 1.5em; font-weight: bold; margin: 30px 0; text-shadow: 1px 1px 2px rgba(0,0,0,0.5); }}
        .stats-hero {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 20px; margin: 30px 0; }}
        .hero-stat {{ background: linear-gradient(135deg, #667eea, #764ba2); color: white; padding: 25px; border-radius: 10px; text-align: center; box-shadow: 0 5px 15px rgba(0,0,0,0.2); }}
        .hero-value {{ font-size: 3em; font-weight: bold; margin: 10px 0; text-shadow: 1px 1px 2px rgba(0,0,0,0.3); }}
        .hero-label {{ font-size: 1em; opacity: 0.9; }}
        .section {{ background: #f8f9fa; padding: 25px; margin: 25px 0; border-radius: 10px; border-left: 5px solid #667eea; }}
        table {{ width: 100%; border-collapse: collapse; margin: 20px 0; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }}
        th {{ background: linear-gradient(135deg, #667eea, #764ba2); color: white; padding: 15px; text-align: left; font-weight: 600; }}
        td {{ padding: 12px; border-bottom: 1px solid #e0e6ed; }}
        tr:hover {{ background: linear-gradient(90deg, #f8f9fa, #e3f2fd); }}
        .ultimate {{ color: #e74c3c; font-weight: bold; font-size: 1.2em; }}
        .success {{ color: #27ae60; font-weight: bold; }}
        .info {{ color: #3498db; }}
        .progress-bar {{ background: #ecf0f1; border-radius: 10px; height: 10px; margin: 5px 0; }}
        .progress-fill {{ background: linear-gradient(90deg, #667eea, #764ba2); height: 100%; border-radius: 10px; transition: width 0.3s; }}
        .footer {{ text-align: center; margin-top: 50px; padding: 30px; background: linear-gradient(135deg, #667eea, #764ba2); color: white; border-radius: 10px; }}
    </style>
</head>
<body>
    <div class="container">
        <h1>ğŸš€ AIäº‹æ¥­ ç©¶æ¥µã®å®Œå…¨ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ</h1>
        
        <div class="ultimate-banner">
            âœ¨ å…¨ {summary['basic_statistics']['total_columns']} ã‚«ãƒ©ãƒ å®Œå…¨åéŒ² âœ¨<br>
            ğŸ¯ åŸºæœ¬å½¢AIäº‹æ¥­ {summary['basic_statistics']['total_projects']} ä»¶ã®å®Œå…¨ãƒ‡ãƒ¼ã‚¿
        </div>
        
        <div class="stats-hero">
            <div class="hero-stat">
                <div class="hero-label">äº‹æ¥­æ•°</div>
                <div class="hero-value">{summary['basic_statistics']['total_projects']}</div>
            </div>
            <div class="hero-stat">
                <div class="hero-label">ç·ã‚«ãƒ©ãƒ æ•°</div>
                <div class="hero-value ultimate">{summary['basic_statistics']['total_columns']}</div>
            </div>
            <div class="hero-stat">
                <div class="hero-label">AIãƒãƒƒãƒæ•°</div>
                <div class="hero-value">{summary['basic_statistics']['total_ai_matches']}</div>
            </div>
            <div class="hero-stat">
                <div class="hero-label">å…¨ä½“å……å®Ÿåº¦</div>
                <div class="hero-value">{summary['completeness_analysis']['overall_completeness']}%</div>
            </div>
        </div>
        
        <div class="section">
            <h2>ğŸ“Š ãƒ‡ãƒ¼ã‚¿å®Œå…¨æ€§åˆ†æ</h2>
            <table>
                <tr>
                    <th>å®Œå…¨æ€§ãƒ¬ãƒ™ãƒ«</th>
                    <th>äº‹æ¥­æ•°</th>
                    <th>å‰²åˆ</th>
                </tr>
                <tr>
                    <td>å®Œå…¨ãƒ‡ãƒ¼ã‚¿ï¼ˆå…¨ã‚«ãƒ©ãƒ ï¼‰</td>
                    <td>{summary['completeness_analysis']['perfect_records']}</td>
                    <td>{round(summary['completeness_analysis']['perfect_records']/summary['basic_statistics']['total_projects']*100, 1)}%</td>
                </tr>
                <tr>
                    <td>é«˜å……å®Ÿåº¦ï¼ˆ80%ä»¥ä¸Šï¼‰</td>
                    <td>{summary['completeness_analysis']['mostly_complete']}</td>
                    <td>{round(summary['completeness_analysis']['mostly_complete']/summary['basic_statistics']['total_projects']*100, 1)}%</td>
                </tr>
                <tr>
                    <td>åŸºæœ¬å……å®Ÿåº¦ï¼ˆ50%ä»¥ä¸Šï¼‰</td>
                    <td>{summary['completeness_analysis']['basic_complete']}</td>
                    <td>{round(summary['completeness_analysis']['basic_complete']/summary['basic_statistics']['total_projects']*100, 1)}%</td>
                </tr>
            </table>
        </div>
        
        <div class="section">
            <h2>ğŸ“ ãƒ†ãƒ¼ãƒ–ãƒ«åˆ¥è©³ç´°çµ±è¨ˆ</h2>
            <table>
                <thead>
                    <tr>
                        <th>ãƒ†ãƒ¼ãƒ–ãƒ«</th>
                        <th>æ—¥æœ¬èªå</th>
                        <th>ã‚«ãƒ†ã‚´ãƒª</th>
                        <th>ã‚«ãƒ©ãƒ æ•°</th>
                        <th>å……å®Ÿåº¦</th>
                        <th>é€²æ—</th>
                    </tr>
                </thead>
                <tbody>"""
        
        for table, stats in summary['table_breakdown'].items():
            coverage = stats['average_coverage']
            html_content += f"""
                    <tr>
                        <td><strong>{table}</strong></td>
                        <td>{stats['japanese_name']}</td>
                        <td>{stats['category']}</td>
                        <td class="ultimate">{stats['total_columns']}</td>
                        <td>{coverage}%</td>
                        <td>
                            <div class="progress-bar">
                                <div class="progress-fill" style="width: {coverage}%"></div>
                            </div>
                        </td>
                    </tr>"""
        
        html_content += f"""
                </tbody>
            </table>
        </div>
        
        <div class="section">
            <h2>ğŸ¢ åºœçœåºåˆ¥åˆ†å¸ƒï¼ˆä¸Šä½10ï¼‰</h2>
            <table>
                <thead>
                    <tr>
                        <th>åºœçœåº</th>
                        <th>äº‹æ¥­æ•°</th>
                        <th>å‰²åˆ</th>
                    </tr>
                </thead>
                <tbody>"""
        
        total_projects = summary['basic_statistics']['total_projects']
        for ministry, count in list(summary.get('ministry_distribution', {}).items())[:10]:
            percentage = round((count / total_projects) * 100, 1)
            html_content += f"""
                    <tr>
                        <td>{ministry}</td>
                        <td>{count}</td>
                        <td>{percentage}%</td>
                    </tr>"""
        
        html_content += f"""
                </tbody>
            </table>
        </div>
        
        <div class="section">
            <h2>ğŸ’¾ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«</h2>
            <div class="stats-hero">
                <div class="hero-stat">
                    <h3>CSVå½¢å¼</h3>
                    <p>ai_ultimate_all_444_columns.csv</p>
                    <p class="ultimate">å…¨{summary['basic_statistics']['total_columns']}ã‚«ãƒ©ãƒ </p>
                </div>
                <div class="hero-stat">
                    <h3>Parquetå½¢å¼</h3>
                    <p>ai_ultimate_all_444_columns.parquet</p>
                    <p class="info">é«˜é€Ÿã‚¢ã‚¯ã‚»ã‚¹ç”¨</p>
                </div>
                <div class="hero-stat">
                    <h3>ã‚«ãƒ©ãƒ ãƒªã‚¹ãƒˆ</h3>
                    <p>ultimate_columns_list.txt</p>
                    <p>å®Œå…¨ã‚«ãƒ©ãƒ ä¸€è¦§</p>
                </div>
            </div>
        </div>
        
        <div class="footer">
            <h2>ğŸ‰ ç©¶æ¥µã®å®Œå…¨ãƒ‡ãƒ¼ã‚¿</h2>
            <p>15ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æŠ½å‡ºã—ãŸå…¨ {summary['basic_statistics']['total_columns']} ã‚«ãƒ©ãƒ ã‚’å®Œå…¨åéŒ²</p>
            <p>åŸºæœ¬å½¢AIäº‹æ¥­ã®æœ€ã‚‚åŒ…æ‹¬çš„ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ</p>
            <p class="ultimate">Generated by AI Ultimate Spreadsheet Generator</p>
        </div>
    </div>
</body>
</html>"""
        
        html_path = self.output_dir / 'ultimate_report.html'
        with open(html_path, 'w', encoding='utf-8') as f:
            f.write(html_content)
        print(f"  HTML report saved: {html_path}")
    
    def run(self):
        """ç©¶æ¥µã®ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆç”Ÿæˆã‚’å®Ÿè¡Œ"""
        print("="*90)
        print("ğŸš€ AIäº‹æ¥­ ç©¶æ¥µã®å®Œå…¨ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆç”Ÿæˆ")
        print("   å…¨444ã‚«ãƒ©ãƒ ã‚’å«ã‚€ç©¶æ¥µã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ")
        print("="*90)
        
        start_time = time.time()
        
        # 1. å…¨ã‚«ãƒ©ãƒ Featherãƒ†ãƒ¼ãƒ–ãƒ«èª­ã¿è¾¼ã¿
        self.load_feather_tables()
        
        if not self.tables_data:
            print("No tables loaded. Exiting.")
            return None
        
        # 2. AIäº‹æ¥­ã®ç‰¹å®š
        ai_projects = self.find_ai_projects()
        
        if not ai_projects:
            print("No AI projects found. Exiting.")
            return None
        
        # 3. ç©¶æ¥µã®ãƒ‡ãƒ¼ã‚¿åé›†ï¼ˆå…¨444ã‚«ãƒ©ãƒ ï¼‰
        ultimate_df = self.collect_ultimate_data(ai_projects)
        
        # 4. ç©¶æ¥µã®ã‚µãƒãƒªãƒ¼ç”Ÿæˆ
        ultimate_summary = self.generate_ultimate_summary(ultimate_df)
        
        # 5. ç©¶æ¥µã®ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆä¿å­˜
        self.save_ultimate_output(ultimate_df, ultimate_summary)
        
        elapsed = time.time() - start_time
        
        print(f"\\n{'='*90}")
        print("ğŸ‰ ç©¶æ¥µã®å®Œå…¨ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆç”Ÿæˆå®Œäº†!")
        print(f"{'='*90}")
        print(f"ğŸ“Š äº‹æ¥­æ•°: {len(ultimate_df):,}è¡Œ")
        print(f"ğŸ“‹ ç·ã‚«ãƒ©ãƒ æ•°: {len(ultimate_df.columns):,}åˆ— (ç©¶æ¥µã®å®Œå…¨ç‰ˆ)")
        print(f"ğŸ¯ AIãƒãƒƒãƒæ•°: {ultimate_summary['basic_statistics']['total_ai_matches']:,}ä»¶")
        print(f"ğŸ“ˆ å…¨ä½“å……å®Ÿåº¦: {ultimate_summary['completeness_analysis']['overall_completeness']}%")
        print(f"â±ï¸  å®Ÿè¡Œæ™‚é–“: {elapsed:.1f}ç§’")
        print(f"ğŸ“ å‡ºåŠ›å…ˆ: {self.output_dir}")
        print(f"{'='*90}")
        
        return ultimate_df, ultimate_summary


if __name__ == "__main__":
    generator = AIUltimateSpreadsheetGenerator()
    generator.run()